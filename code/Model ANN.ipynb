{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Author : Trong Canh Nguyen\n",
    "\n",
    "# This script considers all the products a user has ordered\n",
    "#\n",
    "# We train a model computing the probability of reorder on the \"train\" data\n",
    "#\n",
    "# For the submission, we keep the orders that have a probability of\n",
    "# reorder higher than a threshold\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import gc\n",
    "IDIR = '../input/'\n",
    "FEATURES_PATH = './features3/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(FEATURES_PATH + 'dtypes.pickle', 'rb') as f:\n",
    "    dtype_dict = pickle.load(f)\n",
    "dtype_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data = pd.read_csv(FEATURES_PATH + \"data.csv\", dtype= dtype_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_hdf(FEATURES_PATH + \"data.h5\", \"data\")\n",
    "data.reset_index(inplace=True)\n",
    "print(\"memory = \", data.memory_usage().sum()/1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data[['user_id', 'product_id']].to_hdf(FEATURES_PATH + \"ann_data.h5\", \"user_product_list\", mode = \"a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = ['user_dep_ratio',\n",
    " 'up_orders',\n",
    " 'user_order_size_mean',\n",
    " 'up_add_to_cart_order_mean',\n",
    " 'up_last_order',\n",
    " 'up_order_hour_of_day_mean',\n",
    " 'up_order_rate_since_first_order',\n",
    " 'user_aisle_reordered_ratio',\n",
    " 'user_total_order',\n",
    " 'user_dep_reordered_ratio',\n",
    " 'user_order_hour_of_day',\n",
    " 'up_order_rate',\n",
    " 'user_days_since_prior_mean',\n",
    " 'up_first_order',\n",
    " 'product_reorder_probability',\n",
    " 'up_order_dow_mean',\n",
    " 'up_add_to_cart_order_relative_mean',\n",
    " 'user_reorder_rate',\n",
    " 'user_days_since_prior_order',\n",
    " 'dep_reorder_ratio',\n",
    " 'up_days_since_prior_order_mean',\n",
    " 'up_days_since_last_order',\n",
    " 'aisle_reorder_ratio',\n",
    " 'product_reorder_ratio',\n",
    " 'user_aisle_ratio',\n",
    " 'up_orders_since_last_order',\n",
    " 'user_order_dow']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_train = pd.read_hdf(FEATURES_PATH + \"lgb_data.h5\", \"data_train\")\n",
    "data_valid = pd.read_hdf(FEATURES_PATH + \"lgb_data.h5\", \"data_valid\")\n",
    "data_test = pd.read_hdf(FEATURES_PATH + \"lgb_data.h5\", \"data_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train[['user_id', 'product_id']].set_index(['user_id', 'product_id']).to_hdf(FEATURES_PATH + \"lgb_data.h5\", \"data_train_index\", mode=\"a\")\n",
    "data_valid[['user_id', 'product_id']].set_index(['user_id', 'product_id']).to_hdf(FEATURES_PATH + \"lgb_data.h5\", \"data_valid_index\", mode=\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "columns = list(data_train.columns)\n",
    "not_features = ['user_id', 'product_id', 'up_reordered']\n",
    "features = list(set(columns) - set(not_features))\n",
    "print(\"number of features\", len(features))\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train= data_train[features]\n",
    "y_train= data_train['up_reordered']\n",
    "X_valid= data_valid[features]\n",
    "y_valid= data_valid['up_reordered']\n",
    "X_test = data_test[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save to h5\n",
    "X_train.to_hdf(FEATURES_PATH + \"ann_data.h5\", \"X_train\", mode = \"a\")\n",
    "y_train.to_hdf(FEATURES_PATH + \"ann_data.h5\", \"y_train\", mode = \"a\")\n",
    "X_valid.to_hdf(FEATURES_PATH + \"ann_data.h5\", \"X_valid\", mode = \"a\")\n",
    "y_valid.to_hdf(FEATURES_PATH + \"ann_data.h5\", \"y_valid\", mode = \"a\")\n",
    "X_test.to_hdf(FEATURES_PATH + \"ann_data.h5\", \"X_test\", mode = \"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read from h5\n",
    "X_train = pd.read_hdf(FEATURES_PATH + \"ann_data.h5\", \"X_train\")\n",
    "y_train = pd.read_hdf(FEATURES_PATH + \"ann_data.h5\", \"y_train\")\n",
    "X_valid = pd.read_hdf(FEATURES_PATH + \"ann_data.h5\", \"X_valid\")\n",
    "y_valid = pd.read_hdf(FEATURES_PATH + \"ann_data.h5\", \"y_valid\")\n",
    "X_test = pd.read_hdf(FEATURES_PATH + \"ann_data.h5\", \"X_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "orders = pd.read_csv(IDIR + 'orders.csv', dtype={\n",
    "        'order_id': np.int32,\n",
    "        'user_id': np.int32},\n",
    "        usecols=[\"order_id\", \"user_id\", \"eval_set\"])\n",
    "\n",
    "test_orders= orders[orders.eval_set == 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction = pd.DataFrame()\n",
    "prediction[['user_id', 'product_id']] = user_product_list.loc[X_test.index]\n",
    "prediction['proba'] = pred_test\n",
    "prediction.sort_values(by=['user_id', 'proba'], ascending=[True, False], inplace=True)\n",
    "prediction = pd.merge(prediction, test_orders[['order_id', 'user_id']], on=\"user_id\", how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_product_list.loc[X_test.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendation using threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "threshold = 0.20\n",
    "recommend = prediction[prediction.proba >= threshold].groupby('order_id').product_id.apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "recommend_df = pd.DataFrame()\n",
    "recommend_df[\"count\"] = prediction.groupby('order_id').size()\n",
    "recommend_df['product_list'] = recommend\n",
    "recommend_df['products']= recommend_df.product_list.apply(lambda p: ' '.join([str(x)  for x in p]) if type(p) == list else 'None' ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "recommend_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "min_max_scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_minmax = min_max_scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_valid_minmax = min_max_scaler.transform(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_minmax = min_max_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with h5py.File(FEATURES_PATH + 'ann_data_np.h5', 'w') as hf:\n",
    "    hf.create_dataset(\"X_train_minmax\",  data=X_train_minmax)\n",
    "    hf.create_dataset(\"X_valid_minmax\",  data=X_valid_minmax)\n",
    "    hf.create_dataset(\"X_test_minmax\",  data=X_test_minmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with h5py.File(FEATURES_PATH + 'ann_data_np.h5', 'a') as hf:\n",
    "    hf.create_dataset(\"y_train_value\",  data=y_train.values)\n",
    "    hf.create_dataset(\"y_valid_value\",  data=y_valid.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with h5py.File(FEATURES_PATH+ 'ann_data_np.h5', 'r') as hf:\n",
    "    X_train_values = hf['X_train_minmax'][:]\n",
    "    X_valid_values = hf['X_valid_minmax'][:]\n",
    "    y_train_values = hf['y_train_value'][:]\n",
    "    y_valid_values = hf['y_valid_value'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_valid_index = pd.read_hdf(FEATURES_PATH + \"lgb_data.h5\", \"data_valid_index\").reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 248\n",
    "total_batch = int(len(X_train_values)/batch_size)+1\n",
    "print(\"total batch\", total_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#y_train_onehot = pd.DataFrame(y_train)\n",
    "#y_train_onehot[\"not_up_reordered\"] = 1. - y_train_onehot[\"up_reordered\"]\n",
    "#y_valid_onehot = pd.DataFrame(y_valid)\n",
    "#y_valid_onehot[\"not_up_reordered\"] = 1. - y_valid_onehot[\"up_reordered\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# old version, load from pandas\n",
    "\n",
    "#train_indices = np.array(X_train.index)\n",
    "#def get_batch(indices, batch_number, batch_size):\n",
    "#    i = batch_number\n",
    "#    x_batch = X_train.loc[indices[i*batch_size: (i+1)*batch_size]]\n",
    "#    y_batch = y_train_onehot.loc[indices[i*batch_size: (i+1)*batch_size]]\n",
    "#    return x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot(y):\n",
    "    y_onehot = np.zeros((len(y),2))\n",
    "    y_onehot[:,0] = y\n",
    "    y_onehot[:,1] = 1 - y\n",
    "    return y_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train_onehot_values = one_hot(y_train_values)\n",
    "y_valid_onehot_values = one_hot(y_valid_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_weights = [0.8, 0.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train_onehot_values = y_train_onehot_values*class_weights\n",
    "y_valid_onehot_values = y_valid_onehot_values*class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shuffle_data():\n",
    "    n = len(X_train_values)\n",
    "    indices = np.random.permutation(n)\n",
    "    X_ = X_train_values[indices]\n",
    "    y_ = y_train_onehot_values[indices]\n",
    "    return X_, y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(i, batch_size, X, y):\n",
    "    x_batch = X[i*batch_size: (i+1)*batch_size]\n",
    "    y_batch = y[i*batch_size: (i+1)*batch_size]\n",
    "    return x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create model\n",
    "def create_model(x, weights, biases):\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    \n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    # Output layer with linear activation\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = X_train_values.shape[1]\n",
    "print(\"number of features = \", d)\n",
    "n_input = d # Number of feature\n",
    "n_hidden_1 = 20 # 1st layer number of features\n",
    "n_hidden_2 = 20 # 1st layer number of features\n",
    "\n",
    "n_classes = 2 # Number of classes to predict\n",
    "learning_rate = tf.placeholder(tf.float32, shape=[])\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(tf.float32, [None, n_input])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),   \n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "\n",
    "# Construct model\n",
    "logits = create_model(X, weights, biases)\n",
    "\n",
    "#proba\n",
    "proba = tf.nn.softmax(logits)\n",
    "\n",
    "#manual cross_entropy\n",
    "#coefficients = tf.constant([1.0, 1.0])\n",
    "#eps= tf.constant(value=1e-12)\n",
    "#cost_weighted =  tf.reduce_mean(-tf.reduce_sum( y*tf.log(proba + eps), reduction_indices=[1])) \n",
    "\n",
    "#weight_coeff = tf.constant([0.8, 0.2])\n",
    "#y_weighted = tf.multiply(weight_coeff,  y)\n",
    "#cost_weighted = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels = y_weighted))\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels = y))\n",
    "is_correct =  tf.equal(tf.argmax(proba,1), tf.argmax(y,1))\n",
    "accuracy =  tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "\n",
    "number_batches = total_batch\n",
    "training_epochs = 20\n",
    "display_step = 1\n",
    "display_valid_step = 5\n",
    "\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    lr =  0.0001\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        print(\"lr = \", lr)\n",
    "        lr = lr*0.98\n",
    "        avg_cost = 0.\n",
    "        avg_cost_weighted = 0.\n",
    "        \n",
    "        X_, y_ = shuffle_data()\n",
    "        gc.collect()\n",
    "\n",
    "        # Loop over all batches\n",
    "        for i in range(number_batches):\n",
    "            gc.collect()\n",
    "            batch_x, batch_y = get_batch(i, batch_size, X_, y_)                \n",
    "                \n",
    "            # batch_y.shape = (batch_y.shape[0], 1)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _,  c = sess.run([optimizer, cost], feed_dict={X: batch_x, y: batch_y, learning_rate: lr})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / number_batches\n",
    "            #avg_cost_weighted += c_weighted/number_batches\n",
    "\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print (\"Epoch:\", epoch+1, \"cost= \", avg_cost)\n",
    "        if (epoch+1) % display_valid_step == 0:\n",
    "            print(\"Valid cost:\", sess.run( cost, feed_dict={X: X_valid_values, y: y_valid_onehot_values}) )\n",
    "            #print(\"Valid cost_weighted:\", sess.run( cost_weighted, feed_dict={X: X_valid, y: y_valid_onehot}) )\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Validation\n",
    "   \n",
    "    print(\"Accuracy:\", sess.run( accuracy, feed_dict={X: X_valid_values, y: y_valid_onehot_values}) )\n",
    "    global pred_valid \n",
    "    pred_valid = sess.run( proba, feed_dict={X: X_valid_values}) \n",
    "    #pred_test = sess.run( proba, feed_dict={X: X_test_values})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def precision(y, y_, correct):\n",
    "    if y_>0:\n",
    "        return correct/y_\n",
    "    else:\n",
    "        return 1.0\n",
    "        \n",
    "def recall(y, y_, correct):\n",
    "    if y>0:\n",
    "        return correct/y\n",
    "    else:\n",
    "        return 1.0\n",
    "\n",
    "def f1(y,y_, correct):\n",
    "    p = precision(y, y_, correct)\n",
    "    r = recall(y, y_, correct)\n",
    "    if (p == 0) and (r ==0):\n",
    "        return 0.\n",
    "    f1 = 2*p*r/(p+r)\n",
    "    return f1\n",
    "\n",
    "def compute_f1(valid_df, threshold):\n",
    "    valid_df['y_'] = valid_df['pred'] > threshold\n",
    "    valid_df['correct'] = (valid_df['y'] == valid_df['y_']) & (valid_df['y_'])\n",
    "    result = valid_df.groupby('user_id').sum()\n",
    "    result['f1'] = result.apply(lambda row: f1(row['y'], row['y_'], row['correct']), axis=1)\n",
    "    return result['f1'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1(0,0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_test = pd.read_hdf(FEATURES_PATH + \"lgb_data.h5\", \"data_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_valid = pd.read_hdf(FEATURES_PATH + \"lgb_data.h5\", \"data_valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "valid_df = data_valid[['user_id', 'product_id']].copy()\n",
    "valid_df[\"y\"] = y_valid_values\n",
    "valid_df[\"pred\"] = pred_valid[:,0]\n",
    "valid_df[\"y_\"] = valid_df[\"pred\"]  >= 0.20\n",
    "valid_df['correct'] = (valid_df['y'] == valid_df['y_']) & (valid_df['y_'])\n",
    "valid_df.sort_values(['user_id', 'pred'], ascending=[True, False], inplace = True)\n",
    "#print(\"valid log loss = \", -((valid_df[\"y\"]*np.log(valid_df[\"pred\"])+ (1.-valid_df[\"y\"])* np.log(1.- valid_df[\"pred\"]))).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"valid f1 = \", compute_f1(valid_df, 0.20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#result = valid_df.groupby('user_id').sum()\n",
    "#result['f1'] = result.apply(lambda row: f1(row['y'], row['y_'], row['correct']), axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
