{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Author : Trong Canh Nguyen\n",
    "\n",
    "# This script considers all the products a user has ordered\n",
    "#\n",
    "# We train a model computing the probability of reorder on the \"train\" data\n",
    "#\n",
    "# For the submission, we keep the orders that have a probability of\n",
    "# reorder higher than a threshold\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import gc\n",
    "from helper import *\n",
    "IDIR = '../input/'\n",
    "FEATURES_PATH = './features3/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data = pd.read_csv(FEATURES_PATH + \"data.csv\", dtype= dtype_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_hdf(FEATURES_PATH + \"data.h5\", \"data\")\n",
    "data.reset_index(inplace=True)\n",
    "print(\"memory = \", data.memory_usage().sum()/1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "columns = list(data.columns)\n",
    "not_features = ['user_id', 'product_id', 'aisle_id',  'user_eval_set', 'up_reordered']\n",
    "features = list(set(columns) - set(not_features))\n",
    "print(\"number of features\", len(features))\n",
    "\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Product embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "product_embedding = pd.read_csv(IDIR+\"product_embeddings.csv\")\n",
    "product_embedding.drop(['product_name','aisle_id','department_id'], axis = 1, inplace = True)\n",
    "columns_dict = dict([(str(i), 'pe_'+str(i)) for i in range(32)])\n",
    "product_embedding.rename(columns=columns_dict, inplace = True)\n",
    "pe_columns = ['pe_'+str(i) for i in range(32)]\n",
    "product_embedding[pe_columns] = product_embedding[pe_columns].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = data.merge(product_embedding, on=\"product_id\", how = \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"memory = \", data.memory_usage().sum()/1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.set_index(['user_id', 'product_id'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.to_hdf(FEATURES_PATH + \"data_pe.h5\", \"data\", mode = 'a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data_features = data[data.user_eval_set == \"train\"][['user_id', 'product_id']+ features + [\"up_reordered\"]]\n",
    "data_features = data[data.user_eval_set == \"train\"]\n",
    "\n",
    "tmp = data_features.user_id.unique()\n",
    "user_train = tmp[0:120000]\n",
    "user_valid = tmp[120000:]\n",
    "\n",
    "data_train = data_features[data_features.user_id.isin(user_train)]\n",
    "data_valid = data_features[data_features.user_id.isin(user_valid)]\n",
    "print(len(data_train))\n",
    "print(len(data_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(data_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data_features.to_hdf(FEATURES_PATH + \"lgb_data.h5\", \"data_features\", mode=\"a\")\n",
    "data_train.to_hdf(FEATURES_PATH + \"lgb_data.h5\", \"data_train\", mode=\"a\")\n",
    "data_valid.to_hdf(FEATURES_PATH + \"lgb_data.h5\", \"data_valid\", mode=\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_test = data[data.user_eval_set == \"test\"]\n",
    "data_test.to_hdf(FEATURES_PATH + \"lgb_data.h5\", \"data_test\", mode=\"a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data_train, data_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61\n"
     ]
    }
   ],
   "source": [
    "features = ['up_first_order',\n",
    " 'user_reorder_rate',\n",
    " 'department_id',\n",
    " 'product_reorder_probability',\n",
    " 'product_reorder_ratio',\n",
    " 'up_order_dow_mean',\n",
    " 'user_total_order',\n",
    " 'up_add_to_cart_order_relative_mean',\n",
    " 'dep_reorder_ratio',\n",
    " 'up_orders_since_last_order',\n",
    " 'up_orders',\n",
    " 'user_days_since_prior_order',\n",
    " 'up_add_to_cart_order_mean',\n",
    " 'up_order_rate',\n",
    " 'user_days_since_prior_mean',\n",
    " 'aisle_reorder_ratio',\n",
    " 'user_dep_reordered_ratio',\n",
    " 'user_aisle_reordered_ratio',\n",
    " 'up_last_order',\n",
    " 'up_days_since_prior_order_mean',\n",
    " 'user_dep_ratio',\n",
    " 'up_order_hour_of_day_mean',\n",
    " 'user_order_dow',\n",
    " 'user_order_hour_of_day',\n",
    " 'is_organic',\n",
    " 'user_order_size_mean',\n",
    " 'up_order_rate_since_first_order',\n",
    " 'up_days_since_last_order',\n",
    " 'user_aisle_ratio']\n",
    "\n",
    "features = features + pe_columns\n",
    "\n",
    "print(len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_train len 7757907\n",
      "data_valid len 716754\n"
     ]
    }
   ],
   "source": [
    "data_train = pd.read_hdf(FEATURES_PATH + \"lgb_data.h5\", \"data_train\")\n",
    "data_valid = pd.read_hdf(FEATURES_PATH + \"lgb_data.h5\", \"data_valid\")\n",
    "print(\"data_train len\", len(data_train))\n",
    "print(\"data_valid len\", len(data_valid))\n",
    "\n",
    "#columns = list(data_train.columns)\n",
    "#not_features = ['user_id', 'product_id', 'up_reordered']\n",
    "#features = list(set(columns) - set(not_features))\n",
    "#print(\"number of features\", len(features))\n",
    "#features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train = data_train.merge(product_embedding, on='product_id', how = 'left')\n",
    "data_valid = data_valid.merge(product_embedding, on='product_id', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train= data_train[features].values\n",
    "y_train= data_train['up_reordered'].values\n",
    "X_valid= data_valid[features].values\n",
    "y_valid= data_valid['up_reordered'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del data_train\n",
    "del data_valid\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "with h5py.File(FEATURES_PATH+ 'data_train_product_embedding.h5', 'w') as hf:\n",
    "    hf.create_dataset(\"X_train\",  data=X_train)\n",
    "    hf.create_dataset(\"y_train\",  data=y_train)\n",
    "\n",
    "with h5py.File(FEATURES_PATH+ 'data_train_product_embedding.h5', 'a') as hf:\n",
    "    hf.create_dataset(\"X_valid\",  data=X_valid)\n",
    "    hf.create_dataset(\"y_valid\",  data=y_valid)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weight_train = 1 - (1- y_train)*0.8\n",
    "weight_valid = 1 - (1- y_valid)*0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "formating for lgb\n"
     ]
    }
   ],
   "source": [
    "print('formating for lgb')\n",
    "d_train = lgb.Dataset(X_train, label=y_train, feature_name = features, categorical_feature='department_id')\n",
    "d_valid = lgb.Dataset(X_valid, label=y_valid, feature_name = features, categorical_feature='department_id', reference=d_train)\n",
    "#d_train.save_binary(FEATURES_PATH +  'train.bin')\n",
    "#d_valid.save_binary(FEATURES_PATH +  'valid.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': {'binary_logloss'},\n",
    "    'num_leaves': 500,\n",
    "    'max_depth': 20,\n",
    "    'feature_fraction': 0.5,\n",
    "    'bagging_fraction': 0.95,\n",
    "    'bagging_freq': 5,\n",
    "    'learning_rate':0.1\n",
    "}\n",
    "ROUNDS = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "light GBM train :-)\n"
     ]
    }
   ],
   "source": [
    "print('light GBM train :-)')\n",
    "bst = lgb.train(params, d_train, ROUNDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bst.save_model(FEATURES_PATH + 'lgb/trained_model_pe_2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#bst = lgb.Booster(model_file=FEATURES_PATH+ 'lgb/trained_model_28_features_categorical.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bst.add_valid(d_valid, \"valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('training', 'binary_logloss', 0.22149595873829106, False)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bst.eval_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('valid', 'binary_logloss', 0.24388096216685964, False)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bst.eval_valid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on all dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_all = data_features[features]\n",
    "y_all = data_features['up_reordered']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset_all = lgb.Dataset(X_all, label=y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('light GBM train :-)')\n",
    "bst_all = lgb.train(params, dataset_all, ROUNDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bst_all.save_model(FEATURES_PATH + 'lgb/trained_model_num_leaves500_ALL.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bst_all.eval_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def precision(y, y_, correct, has_none=False):\n",
    "    if y_ > 0:\n",
    "        if not has_none:\n",
    "            return correct / y_\n",
    "        else:\n",
    "            if y > 0:\n",
    "                return correct / (y_ + 1)\n",
    "            else:\n",
    "                return 1. / (y_ + 1)\n",
    "    else:\n",
    "        return 1.0\n",
    "\n",
    "\n",
    "def recall(y, y_, correct, has_none=False):\n",
    "    if y > 0:\n",
    "        return correct / y\n",
    "    else:\n",
    "        if has_none or (y_ == 0):\n",
    "            return 1.0\n",
    "        else:\n",
    "            return 0.\n",
    "\n",
    "\n",
    "def f1(y, y_, correct, has_none=False):\n",
    "    p = precision(y, y_, correct, has_none)\n",
    "    r = recall(y, y_, correct, has_none)\n",
    "    if (p == 0) and (r == 0):\n",
    "        return 0.\n",
    "    f1 = 2 * p * r / (p + r)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def compute_f1(valid_df, threshold):\n",
    "    valid_df['y_'] = valid_df['pred'] > threshold\n",
    "    valid_df['correct'] = (valid_df['y'] == valid_df['y_']) & (valid_df['y_'])\n",
    "    result = valid_df.groupby('user_id').sum()\n",
    "    result['f1'] = result.apply(lambda row: f1(row['y'], row['y_'], row['correct']), axis=1)\n",
    "    return result['f1'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#bst = lgb.Booster(model_file=FEATURES_PATH+ 'lgb/trained_model_27_features_3.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_valid = bst.predict(X_valid)\n",
    "pred_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "valid_df = data_valid[['user_id', 'product_id']].copy()\n",
    "valid_df[\"y\"] = y_valid\n",
    "valid_df[\"pred\"] = pred_valid\n",
    "valid_df[\"y_\"] = valid_df[\"pred\"]  >= 0.2\n",
    "valid_df['correct'] = (valid_df['y'] == valid_df['y_']) & (valid_df['y_'])\n",
    "valid_df.sort_values(['user_id', 'pred'], ascending=[True, False], inplace = True)\n",
    "#print(\"valid log loss = \", -((valid_df[\"y\"]*np.log(valid_df[\"pred\"])+ (1.-valid_df[\"y\"])* np.log(1.- valid_df[\"pred\"]))).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "compute_f1(valid_df, 0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "valid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = valid_df.groupby('user_id').sum()\n",
    "result['precision'] = result.apply(lambda row: precision(row['y'], row['y_'], row['correct']), axis=1)\n",
    "result['recall'] = result.apply(lambda row: recall(row['y'], row['y_'], row['correct']), axis=1)\n",
    "result['f1'] = result.apply(lambda row: f1(row['y'], row['y_'], row['correct']), axis=1)\n",
    "\n",
    "print(\"precision mean = \", result.precision.mean())\n",
    "print(\"recall mean = \", result.recall.mean())\n",
    "print(\"f1 mean = \", result.f1.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#valid_df.to_csv(FEATURES_PATH+ \"valid_df.csv\")\n",
    "valid_df.to_hdf(FEATURES_PATH+ \"results.h5\", \"valid_df\", mode = \"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "valid_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#bst_all = lgb.Booster(model_file=FEATURES_PATH+ 'trained_model.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_test = pd.read_hdf(FEATURES_PATH + \"lgb_data.h5\", \"data_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('light GBM predict')\n",
    "pred_test = bst.predict(data_test[features])\n",
    "pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "orders = pd.read_csv(IDIR + 'orders.csv', dtype={\n",
    "        'order_id': np.int32,\n",
    "        'user_id': np.int32},\n",
    "        usecols=[\"order_id\", \"user_id\", \"eval_set\"])\n",
    "\n",
    "test_orders= orders[orders.eval_set == 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction = pd.DataFrame()\n",
    "prediction[['user_id', 'product_id']] = data_test[['user_id', 'product_id']]\n",
    "prediction['proba'] = pred_test\n",
    "prediction.sort_values(by=['user_id', 'proba'], ascending=[True, False], inplace=True)\n",
    "prediction = pd.merge(prediction, test_orders[['order_id', 'user_id']], on=\"user_id\", how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Plot feature importances...')\n",
    "ax = lgb.plot_importance(bst, max_num_features=50)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendation using threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "threshold = 0.20\n",
    "recommend = prediction[prediction.proba >= threshold].groupby('order_id').product_id.apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "recommend.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "add_none_df = prediction[prediction.proba >= threshold].groupby('order_id').proba.agg([np.size, np.max])\n",
    "add_none_df['None'] = (add_none_df['size'] > 0) & (add_none_df['size'] < 4) & (add_none_df['amax'] < 0.46)\n",
    "add_none_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "recommend_df = pd.DataFrame()\n",
    "recommend_df[\"count\"] = prediction.groupby('order_id').size()\n",
    "recommend_df['product_list'] = recommend\n",
    "recommend_df['none'] = add_none_df['None']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "recommend_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_prediction(row):\n",
    "    p = row.product_list\n",
    "    if type(p) == list:\n",
    "        result = ' '.join([str(x)  for x in p])\n",
    "        \n",
    "        if row.none:\n",
    "            result = 'None ' + result\n",
    "        \n",
    "        return result\n",
    "    else:\n",
    "        return 'None'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "recommend_df['products']= recommend_df.apply(lambda row:  generate_prediction(row) , axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "recommend_df['products']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "recommend_df['products'].to_csv(FEATURES_PATH +  'lgb/recommend5_none.csv', header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendation using average user basket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "products = []\n",
    "count = 0\n",
    "for _,row in test_orders[['user_id', 'order_id']].iterrows():\n",
    "    count += 1\n",
    "    if (count)%10000 == 0:\n",
    "        print(count)    \n",
    "    \n",
    "    user_id, order_id = row['user_id'], row['order_id']\n",
    "    n = int(user_basket_avg.ix[user_id].basket_size_avg)+1\n",
    "    products.append(list(prediction[prediction.user_id == user_id].product_id[:n]))\n",
    "    \n",
    "# create submission\n",
    "submission = pd.DataFrame()\n",
    "submission['order_id'] = test_orders['order_id']\n",
    "submission['products'] = [' '.join([str(x) for x in p]) for p in products]\n",
    "submission.sort_values(by='order_id', inplace = True)\n",
    "submission.to_csv(FEATURES_PATH + 'submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "valid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "users_none_incorrect = tmp[(tmp.y == 0) &  (tmp.y_ > 0)].user_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "valid_df[valid_df.user_id.isin(users_none_incorrect) & valid_df.y_].pred.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "valid_df[valid_df.y_].pred.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"None predicted correctly = \", len(tmp[(tmp.y == 0) &  (tmp.y_ == 0)]) / len(tmp[tmp.y == 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"precision mean = \", result.precision.mean())\n",
    "print(\"recall mean = \", result.recall.mean())\n",
    "print(\"f1 mean = \", result.f1.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = 0.3934358303379222\n",
    "r = 0.5387833182445639\n",
    "2*p*r/(p+r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Not None but predict None = \", len(tmp[(tmp.y > 0) &  (tmp.y_ == 0)]) / len(tmp[tmp.y > 0]))\n",
    "print(\"None but predicted not None = \", len(tmp[(tmp.y == 0) &  (tmp.y_ > 0)]) / len(tmp[tmp.y == 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "users_valid_not_reorder = tmp[(tmp.y == 0)].reset_index().user_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_valid_user_not_reorder = data_valid[data_valid.user_id.isin(users_valid_not_reorder)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_valid_user_not_reorder.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_info = pd.read_hdf(FEATURES_PATH + \"features.h5\", \"user_info\").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_info_user_not_reorder = user_info[user_info.user_id.isin(users_valid_not_reorder)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_info_user_not_reorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trains = pd.read_hdf(IDIR + \"input.h5\", \"trains\")\n",
    "trains = trains.merge(orders, on= \"order_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trains[trains.user_id == 188603]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "valid_df[valid_df.user_id == 188603]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f1_score(p,q):\n",
    "    return 2*p*q/(p+q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f1_score(1, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp[tmp.y == 0].y_.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When there is None, we predict on average 2.8542780748663104 products ==> can be improved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_none_false =  len(tmp[(tmp.y == 0) &  (tmp.y_ > 0)]) \n",
    "nb_none_true = len(tmp[(tmp.y == 0) &  (tmp.y_ ==  0)]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nb_none_false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nb_none_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nb_none = len(tmp[(tmp.y == 0)])\n",
    "nb_none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nb_valid=  len(tmp)\n",
    "nb_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_f1 = 0.38*11209\n",
    "total_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_f1_improve = nb_none_false*0.67*0.\n",
    "total_f1_improve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_f1 = (total_f1 + total_f1_improve)/nb_valid\n",
    "new_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp[(tmp.y_ == 1) & (tmp.y == 1) & (tmp.correct == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp[(tmp.y_ == 1) & (tmp.correct == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp[(tmp.y_ == 1) & (tmp.y == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f1_score(1,1) - f1_score(1/2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f1_score(1,1/2) - f1_score(1/2,1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f1_score(1,1/3) - f1_score(1/2,1/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f1_score(1/2,1) - 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "valid_df = data_valid[['user_id', 'product_id']].copy()\n",
    "valid_df[\"y\"] = y_valid\n",
    "valid_df[\"pred\"] = pred_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "valid_df[\"y_\"] = valid_df[\"pred\"]  >= 0.2\n",
    "valid_df['correct'] = (valid_df['y'] == valid_df['y_']) & (valid_df['y_'])\n",
    "valid_df.sort_values(['user_id', 'pred'], ascending=[True, False], inplace = True)\n",
    "valid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "threshold_none = 0.45\n",
    "result = valid_df.groupby('user_id').agg({'y':np.sum,'y_':np.sum, 'correct':np.sum, 'pred':np.max}).reset_index()\n",
    "result['None'] = (result['y_'] > 0) & (result['y_'] < 4) &(result['pred'] < threshold_none)\n",
    "result['precision'] = result.apply(lambda row: precision(row['y'], row['y_'], row['correct'], row['None']), axis=1)\n",
    "result['recall'] = result.apply(lambda row: recall(row['y'], row['y_'], row['correct'], row['None']), axis=1)\n",
    "result['f1'] = result.apply(lambda row: f1(row['y'], row['y_'], row['correct'], row['None']), axis=1)\n",
    "print(result.f1.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result[(result.y > 0) & (result.correct == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result[(result.y == 0) & (result.y_ == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(result.f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result[(result.y ==0) & (result.y_ > 0) & (result['None'] == False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "459 + 1777 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(result[(result.f1 == 0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "valid_df[valid_df.user_id == 188562]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_valid[(data_valid.user_id == 188562) & (data_valid.product_id  == 43409)].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
